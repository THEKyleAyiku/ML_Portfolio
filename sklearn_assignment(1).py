# -*- coding: utf-8 -*-
"""sklearn Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zldpc-PKKEQ4Nfc3IkD56XH_OhXdjC03
"""

# Reading in the data set (Part 1)
from google.colab import files
uploaded = files.upload()
import io
import pandas as pd
auto = pd.read_csv(io.BytesIO(uploaded['Auto.csv']))
print(auto.head()) # First few rows (the head).
print('\nDimensions of data frame:', auto.shape)

# Data Exploration (Part 2)
column_one = 'mpg' # First one is mpg.
mpg_desc = auto[column_one].describe()
print(mpg_desc)

column_two = 'weight' # Second one is weight.
weight_desc = auto[column_two].describe()
print(weight_desc)

column_three = 'year' # Third one is year,
year_desc = auto[column_three].describe()
print(year_desc)

# Exploring Data Types (Part 3)
auto.dtypes # Checking the data types of all columns.

auto_1 = auto.copy() # Changing 'cylinders' to categorical (cat.codes)
auto_1.cylinders = auto_1.cylinders.astype('category').cat.codes
print(auto_1.dtypes, "\n")
print(auto_1.head())

auto_2 = auto.copy() # Changing 'origin' to categorical
auto_2.origin = auto_2.origin.astype('category')
print(auto_2.dtypes, "\n")
print(auto_2.head())

# Dealing w/ NAs (Part 4)
auto.isnull().sum() # Checking for any NAs
auto = auto.dropna() # Deleting the rows w/ NAs
print('\nDimensions of data frame:', auto.shape)

# Modifying columns (Part 5)
import numpy as np # Creating 'mpg_high'.
auto['mpg_high'] = np.where(auto.mpg > np.mean(auto.mpg), 1, 0)
auto = auto.drop(columns = ['mpg', 'name'])
print(auto.head())

# Graph Data Exploration (Part 6)
import seaborn as sb 
sb.catplot(x = 'mpg_high', kind = 'count', data = auto)
    # Catplot for the 'mpg_high' column.
sb.relplot(x = 'horsepower', y = 'weight', data = auto, hue = auto.mpg_high)
    # Relplot for the columns 'horsepower' & 'weight'.

# Graph Data Exploration (Part 6, cont.)
sb.boxplot(x = 'mpg_high', y = 'weight', data = auto)

# Train/test Splitting (Part 7)
from sklearn.model_selection import train_test_split

X = auto.iloc[:, 0:6]
y = auto.iloc[:, 7]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,
                                                    random_state = 1234)

print('train size:', X_train.shape) # Dimensions for both train and test.
print('test_size:', X_test.shape)

, f1_score
, recall_score
# Logistic Regression (Part 8)
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(solver = 'lbfgs')
clf.fit(X_train, y_train)
print(clf.score(X_train, y_train)) # Output is down below.

pred_1 = clf.predict(X_test) # Testing and evaluating.
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
     f1_score)
print('accuracy score: ', accuracy_score(y_test, pred_1))
print('precision score: ', precision_score(y_test, pred_1))
print('recall score: ', recall_score(y_test, pred_1))
print('f1_score: ', f1_score(y_test, pred_1))

from sklearn.metrics import classification_report
print(classification_report(y_test, pred_1))

# Decision Tree (Part 9)
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)
print(clf.score(X_train, y_train))

pred_2 = clf.predict(X_test) # Testing and evaluating.
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
     f1_score)
print('accuracy score: ', accuracy_score(y_test, pred_2))
print('precision score: ', precision_score(y_test, pred_2))
print('recall score: ', recall_score(y_test, pred_2))
print('f1_score: ', f1_score(y_test, pred_2))

from sklearn.metrics import classification_report
print(classification_report(y_test, pred_2))

# Scaling the data for Neural Networks
from sklearn import preprocessing

scaler = preprocessing.StandardScaler().fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# Neural Networks (Part 10)
from sklearn.neural_network import MLPRegressor
regr = MLPRegressor(hidden_layer_sizes = (6, 3), max_iter = 500, # Network 1.
                    random_state = 1234)
regr.fit(X_train, y_train)
pred_3 = regr.predict(X_test) # Predicting (MLPRegressor)

from sklearn.metrics import mean_squared_error, r2_score
print('mse = ', mean_squared_error(y_test, pred_3))
print('correlation = ', r2_score(y_test, pred_3))


from sklearn.neural_network import MLPClassifier
classif = MLPClassifier(hidden_layer_sizes = (5, 2), solver = 'lbfgs', 
                        max_iter = 1500,
                        random_state = 1234) # Network 2.
classif.fit(X_train, y_train)
pred_4 = classif.predict(X_test) # Predicting (MLPClassifier)

from sklearn.metrics import mean_squared_error, r2_score
print('mse = ', mean_squared_error(y_test, pred_4))
print('correlation = ', r2_score(y_test, pred_4))

"""Comparison: For the MLP neural networks, I believe the MLPClassifier performed better since there is a positive correlation of 0.3314, and the MSE is lower than MLPRegressor's.

# Analysis (Part 11)
Analysis: I believe the decision tree algorithm performed better because the CLP score is 1.0 whereas logistic regression's is about 0.894, which is lower than the former's algorithm. Trufhfully, I still prefer utilizing R for the machine learning algorithms over sklearn for Python since I am more used to the former's language and syntax. However, I enjoyed implementing regression and classification techniques for both as for Python, I can just import packages to assist with implementations whereas for R, I have to install numerous packages and the performance is much slower compared to sklearn.
"""